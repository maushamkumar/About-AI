{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e62f90",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "pip install langchain-mistralai langchain-core python-dotenv\n",
    "\n",
    "Mistralai API KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c649605",
   "metadata": {},
   "source": [
    "### Basic Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ea616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35e2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d893db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INitialize Mistral LLM \n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\", \n",
    "    mistral_api_key = mistral_api_key,\n",
    "    temperature=0.7, \n",
    "    max_tokens=400, \n",
    "    top_p=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8ab0d",
   "metadata": {},
   "source": [
    "### Basic LLMchain Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f23546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunoke Prompt template \n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['text'], \n",
    "    template=\"Summarize the following text in 3 bullet points:\\n\\n{text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e193d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/8rqwg0nd7995t2tzgq40dsph0000gn/T/ipykernel_16858/298678160.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
     ]
    }
   ],
   "source": [
    "# Create chain \n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5221d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/8rqwg0nd7995t2tzgq40dsph0000gn/T/ipykernel_16858/1033691104.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(text=text_to_summarize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **AI Automation and Insights**: Artificial Intelligence has transformed industries by automating complex tasks and deriving insights from large datasets.\n",
      "- **Applications**: Machine learning algorithms are now capable of predicting market trends, diagnosing diseases, and creating art.\n",
      "- **Concerns**: The rapid progress of AI also brings worries about potential job displacement and the ethical implications of AI-driven decisions.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_to_summarize = \"\"\"\n",
    "Artificial Intelligence has revolutionized various industries by automating \n",
    "complex tasks and providing insights from large datasets. Machine learning \n",
    "algorithms can now predict market trends, diagnose diseases, and even create \n",
    "art. However, this rapid advancement also raises concerns about job displacement \n",
    "and ethical implications of AI decision-making.\n",
    "\"\"\"\n",
    "\n",
    "result = chain.run(text=text_to_summarize)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e37bc",
   "metadata": {},
   "source": [
    "### Advance Chain with Chat prompts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb896268",
   "metadata": {},
   "source": [
    "### Experimenting with LLM Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62484d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat-based prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert {domain} analyst with 10 years of experience.\"),\n",
    "    (\"human\", \"Please analyze the following scenario and provide actionable insights:\\n{scenario}\")\n",
    "])\n",
    "\n",
    "# Create advanced chain\n",
    "advanced_chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "# Example usage\n",
    "result = advanced_chain.run(\n",
    "    domain=\"financial\",\n",
    "    scenario=\"A tech startup is considering IPO but market conditions are volatile.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aabadbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Given the scenario where a tech startup is considering an Initial Public Offering (IPO) but market conditions are volatile, here are some actionable insights and considerations:\\n\\n### 1. **Market Timing**\\n   - **Volatility Assessment**: Analyze the sources of market volatility (e.g., geopolitical events, economic indicators, sector-specific trends). Historical data can provide insights into how similar volatility periods have affected IPO performance.\\n   - **Wait and Watch**: If the volatility is expected to be short-term, it might be prudent to delay the IPO until market conditions stabilize.\\n   - **Window of Opportunity**: Identify potential windows of stability within the volatile market. Sometimes, even in volatile conditions, there are brief periods of calm that can be leveraged.\\n\\n### 2. **Valuation Considerations**\\n   - **Conservative Valuation**: In volatile markets, investors might be more risk-averse. Consider a conservative valuation to attract investors and ensure a successful IPO.\\n   - **Comparable Analysis**: Look at recent IPOs in the tech sector to understand how similar companies have been valued and performed post-IPO.\\n\\n### 3. **Investor Sentiment**\\n   - **Roadshow Strategy**: Engage with potential investors through a well-planned roadshow. Highlight the company's unique value proposition, growth potential, and resilience to market volatility.\\n   - **Diversified Investor Base**: Target a diverse range of investors, including institutional investors, retail investors, and strategic partners who might be less sensitive to short-term market fluctuations.\\n\\n### 4. **Risk Management**\\n   - **Hedging Strategies**: Consider financial instruments to hedge against market risks. This could include\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23bb60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbfcdfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=os.getenv(\"GORQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d54e39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f87d1f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature: 0.1 (Very focused, deterministic)\n",
      "Output: {'text': '\"As the last remnants of sunlight faded from the rav'}\n",
      "\n",
      "Temperature: 0.6 (Balanced creativity and focus)\n",
      "Output: {'text': '\"As the last remnants of sunlight faded from the rav'}\n",
      "\n",
      "Temperature: 0.9 (Highly creative, less predictable)\n",
      "Output: {'text': '\"As the last star in the galaxy died, a'}\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "def experiment_with_parameters():\n",
    "    \"\"\"Test different LLM parameters to understand their impact\"\"\"\n",
    "    \n",
    "    scenarios = [\n",
    "        {\"temperature\": 0.1, \"description\": \"Very focused, deterministic\"},\n",
    "        {\"temperature\": 0.6, \"description\": \"Balanced creativity and focus\"},\n",
    "        {\"temperature\": 0.9, \"description\": \"Highly creative, less predictable\"}\n",
    "    ]\n",
    "    \n",
    "    prompt = \"Write a creative opening line for a sci-fi novel.\"\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        llm_variant = ChatGroq(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\"), \n",
    "            temperature=scenario[\"temperature\"], \n",
    "            max_tokens=10,\n",
    "        )\n",
    "        \n",
    "        chain = LLMChain(\n",
    "            llm=llm_variant,\n",
    "            prompt=PromptTemplate(input_variables=[], template=prompt)\n",
    "        )\n",
    "        \n",
    "        result = chain.invoke({})\n",
    "        print(f\"\\nTemperature: {scenario['temperature']} ({scenario['description']})\")\n",
    "        print(f\"Output: {result}\")\n",
    "\n",
    "# Run the experiment\n",
    "experiment_with_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8aef3152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca632b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/8rqwg0nd7995t2tzgq40dsph0000gn/T/ipykernel_19823/3401281750.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm_variant, prompt=prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\"As the last star in the Andromeda galaxy flickered out, a lone message beacon, sent centuries ago by a long-extinct civilization, finally reached its destination on a desolate planet called New Eden, where the remnants of humanity huddled in the ruins of a forgotten city, awaiting the arrival of the inevitable darkness that would soon consume them all.\"'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "\n",
    "llm_variant = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",  # ✅ Correct model name\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),  # ✅ Securely using .env\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[], template=\"Write a creative opening line for a sci-fi novel.\")\n",
    "\n",
    "chain = LLMChain(llm=llm_variant, prompt=prompt)\n",
    "output = chain.invoke({})\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91d7fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce495813",
   "metadata": {},
   "source": [
    "#### 1. Zero-shot Prompting \n",
    "The most basic from - direct instruction without exampels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a22b121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaymahto/About-AI/.venv/lib/python3.11/site-packages/langchain_groq/chat_models.py:364: UserWarning: WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\"), \n",
    "            temperature=0.7, \n",
    "            max_tokens=200,\n",
    "            top_p=1.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07aefeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=['taks', 'input'], \n",
    "    template=\"\"\"\n",
    "    Task:{task}\n",
    "    Input: {input}\n",
    "    Output:\n",
    "    \"\"\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7965666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "chain = LLMChain(llm = llm, prompt=zero_shot_prompt)\n",
    "result = chain.run(\n",
    "    task=\"Classify the sentiment of this review as positive, negative, or neutral\",\n",
    "    input=\"Product thik thak tha.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba0bac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This review is difficult to classify as it is not in a conventional language and does not contain any explicit sentiment-bearing words. However, based on the cultural context and the product name \"thik thak tha\", I would classify this review as neutral.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a8146",
   "metadata": {},
   "source": [
    "### 2. Few-shot Prompting \n",
    "Provide examples to guide the model's behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bac790d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = PromptTemplate(\n",
    "    input_variables=['input'], \n",
    "    template=\"\"\"\n",
    "    Classify the sentimaent of product reviews:\n",
    "    \n",
    "    Review: \"This product is amazing! Best purchase ever!\"\n",
    "    Sentiment: Positive\n",
    "    \n",
    "    Review: \"Terrible quality, broke after one day.\"\n",
    "    Sentiment: Negative\n",
    "    \n",
    "    Review: \"It's okay, does what it's supposed to do.\"\n",
    "    Sentiment: Neutral\n",
    "\n",
    "    Review: \"{input}\"\n",
    "    Sentiment:    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f31cc",
   "metadata": {},
   "source": [
    "### 3. Chain-of-Thought (CoT) Prompting \n",
    "Encourage step-by-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a54c15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = PromptTemplate(\n",
    "    input_variables=['problem'], \n",
    "    template=\"\"\"\n",
    "    Solve this problem step by step:\n",
    "    \n",
    "    Problem: {problem}\n",
    "    \n",
    "    Let's think through this step by step:\n",
    "    1. First, I need to identiry what we're looking for \n",
    "    2. Then, I'll break down the problem into smaller parts \n",
    "    3. Finally, I'll solve each part and combine the results\n",
    "    \n",
    "    Step 1:\n",
    "     \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8856b",
   "metadata": {},
   "source": [
    "### 4. Role-Based Prompting \n",
    "Assign specific roles or personas to the AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7756cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_based_prompt = PromptTemplate(\n",
    "    input_variables=['role', 'task', 'context'],\n",
    "    template=\"\"\"\n",
    "    You are a {role} with extensive experience in your field.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Task: {task}\n",
    "    \n",
    "    please provide your expert analysis and recommendations:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0851acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=role_based_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71562d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.run(\n",
    "    role=\"senior software architect\",\n",
    "    context=\"E-commerce platform expecting 10x traffic growth in 6 months\",\n",
    "    task=\"Review this system design for scalability issues\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb60e0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a senior software architect, I'll provide a thorough review of the system design for scalability issues and offer recommendations to ensure the e-commerce platform can handle the expected 10x traffic growth in 6 months.\n",
      "\n",
      "**System Design Review**\n",
      "\n",
      "The system design appears to be a typical e-commerce platform architecture, consisting of the following components:\n",
      "\n",
      "1. **Web Application**: A web application built using a web framework (e.g., Spring, Django) that handles user requests, processes orders, and updates the database.\n",
      "2. **Database**: A relational database management system (RDBMS) like MySQL or PostgreSQL that stores product information, customer data, and order details.\n",
      "3. **Cache Layer**: An in-memory data grid (e.g., Redis, Hazelcast) that caches frequently accessed data to reduce the load on the database.\n",
      "4. **Message Queue**: A message broker (e.g., RabbitMQ, Apache Kafka) that handles asynchronous tasks, such as sending order notifications and processing payments.\n",
      "5.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359b0c0",
   "metadata": {},
   "source": [
    "### 5. Constraint-Based Prompting \n",
    "Set specific limitations or requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b45c5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_prompt = PromptTemplate(\n",
    "    input_variables=['topic', 'constraints'],\n",
    "    template=\"\"\"\n",
    "    Write about {topic} following these constraints:\n",
    "    {contraints}\n",
    "    \n",
    "    content:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fa03cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=constraint_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01cffc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Humman Ke Bare (About Humans)\\n\\nHumans are fascinating creatures! They are capable of incredible feats, from exploring space to creating beautiful works of art. But what makes humans so unique? Let's take a look at some examples.\\n\\nFor instance, humans have an incredible capacity for language. We have developed over 7,000 languages, each with its own set of rules and dialects. Imagine being able to communicate with someone in a completely new way, without relying on words! \\n\\nAnother example is their capacity for creativity. Humans have created some of the most breathtaking works of art, from the Mona Lisa to the Taj Mahal. They have also invented incredible technologies, like the internet and smartphones.\\n\\nBut what about their capacity for kindness and compassion? Humans have shown incredible resilience in the face of adversity, from helping each other during natural disasters to working together to end conflicts.\\n\\nWhat do you think makes humans so incredible?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\n",
    "    topic = \"Humman ke bare me\", \n",
    "    contraints = \"\"\"\n",
    "    - Maximum 150 words\n",
    "    - Include at least 3 specific examples \n",
    "    - Use simple language (8th grade reading level)\n",
    "    -End with a question to engage the read \n",
    "    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e92eea",
   "metadata": {},
   "source": [
    "### Template-Based Prompting \n",
    "Use structured formates for consistent outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "097cc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_prompt = PromptTemplate(\n",
    "    input_variables=['product', 'features'], \n",
    "    template=\"\"\"\n",
    "    Create a product description using this template:\n",
    "    \n",
    "    PRODUCT: {product}\n",
    "    FEATURES: {features}\n",
    "    \n",
    "    Templates:\n",
    "    **Headline**: [Catchy product name and main benefit]\n",
    "    **Problem**: [What problem does this solve?]\n",
    "    **Solution**: [How does the product solve it]\n",
    "    **Features**: [List 3 key features with benefits]\n",
    "    **Call to Actions**: [Compelling action statement]\n",
    "    \n",
    "    Fill out the template:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd576b7",
   "metadata": {},
   "source": [
    "### 7. Iterative Refinement Prompting\n",
    "Build upon previous responses for better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cee3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prompting(initial_prompt, refinement_instructions, iterations=5):\n",
    "    \"\"\"Implement iterative refinement prompting\"\"\"\n",
    "    \n",
    "    current_output = None\n",
    "\n",
    "    for i in range(iterations):\n",
    "        if i == 0:\n",
    "            chain = LLMChain(llm=llm, prompt=PromptTemplate(\n",
    "                input_variables=[], \n",
    "                template=initial_prompt\n",
    "            ))\n",
    "            current_output = chain.run({})\n",
    "\n",
    "        else:\n",
    "            refine_prompt = PromptTemplate(\n",
    "                input_variables=['previous_output', 'instructions'], \n",
    "                template=\"\"\"\n",
    "                Previous output:\n",
    "                {previous_output}\n",
    "                \n",
    "                Please improve the above output based on these instructions:\n",
    "                {instructions}\n",
    "                \n",
    "                Improved output:\n",
    "                \"\"\"\n",
    "            )\n",
    "            chain = LLMChain(llm=llm, prompt=refine_prompt)\n",
    "            current_output = chain.run(\n",
    "                previous_output=current_output, \n",
    "                instructions=refinement_instructions\n",
    "            )\n",
    "            \n",
    "            print(f'\\nIteration {i+1}:')\n",
    "            print(current_output)\n",
    "\n",
    "    return current_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f083b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2:\n",
      "Here's an improved version of the output:\n",
      "\n",
      "                Recycling is a crucial practice that plays a vital role in safeguarding our planet's future and ensuring the well-being of our communities. By recycling, we can significantly reduce the staggering amounts of waste that threaten our environment and public health, and instead, preserve the natural resources we need to thrive. Recycling not only conserves precious water, energy, and raw materials, but also helps to reduce the environmental footprint of production, transportation, and disposal. Moreover, recycling is a powerful tool in the fight against climate change, as it decreases the demand for resource-intensive production processes and reduces the emissions that perpetuate this crisis. By making recycling a habit, we can create a cleaner, healthier, and more sustainable world for ourselves and future generations. So, let's join forces to make a difference! Take the pledge to recycle, and together, we can create a better tomorrow.\n",
      "\n",
      "I've made the following changes to improve the output:\n",
      "\n",
      "1. Added stronger language to\n",
      "\n",
      "Iteration 3:\n",
      "Here's an improved version of the output:\n",
      "\n",
      "**Join the Movement to Save Our Planet: Make Recycling a Habit Today!**\n",
      "\n",
      "Recycling is not just a responsible practice, it's a crucial step towards ensuring a sustainable future for our planet and its inhabitants. By recycling, we can dramatically reduce the staggering amounts of waste that threaten our environment, public health, and the well-being of our communities. Think of the impact we can make: conserving precious water, energy, and raw materials, reducing the environmental footprint of production, transportation, and disposal, and fighting climate change by decreasing the demand for resource-intensive production processes.\n",
      "\n",
      "**The Stakes Are High, But So Are the Benefits**\n",
      "\n",
      "By recycling, we can:\n",
      "\n",
      "* Protect our air and water from pollution\n",
      "* Conserve natural resources for future generations\n",
      "* Reduce greenhouse gas emissions and slow climate change\n",
      "* Create jobs and stimulate local economies\n",
      "* Build a healthier, more sustainable community for ourselves and our children\n",
      "\n",
      "**So What Can You Do?\n",
      "\n",
      "Iteration 4:\n",
      "Here's a revised version of the output:\n",
      "\n",
      "**Join the Movement to Save Our Planet: Make Recycling a Habit Today!**\n",
      "\n",
      "The fate of our planet is in our hands. Recycling is not just a responsible practice, it's a matter of survival. By recycling, we can dramatically reduce the staggering amounts of waste that threaten our environment, public health, and the well-being of our communities. Think of the impact we can make: conserving precious water, energy, and raw materials, reducing the environmental footprint of production, transportation, and disposal, and fighting climate change by decreasing the demand for resource-intensive production processes.\n",
      "\n",
      "**The Stakes Are Higher Than Ever**\n",
      "\n",
      "The consequences of inaction are dire. Rising temperatures, devastating natural disasters, and unpredictable weather patterns are all symptoms of a planet in crisis. But the good news is that recycling can be a game-changer. By recycling, we can:\n",
      "\n",
      "* Protect our air and water from pollution, ensuring a healthy environment for generations to come\n",
      "* Con\n",
      "\n",
      "Iteration 5:\n",
      "Here's a revised version of the output:\n",
      "\n",
      "**Join the Movement to Save Our Planet: Make Recycling a Habit Today and Secure a Sustainable Future!**\n",
      "\n",
      "The fate of our planet is in our hands. Recycling is not just a responsible practice, it's a matter of survival. By recycling, we can dramatically reduce the staggering amounts of waste that threaten our environment, public health, and the well-being of our communities. Think of the impact we can make: conserving precious water, energy, and raw materials, reducing the environmental footprint of production, transportation, and disposal, and fighting climate change by decreasing the demand for resource-intensive production processes.\n",
      "\n",
      "**The Stakes Are Higher Than Ever**\n",
      "\n",
      "The consequences of inaction are dire. Rising temperatures, devastating natural disasters, and unpredictable weather patterns are all symptoms of a planet in crisis. But the good news is that recycling can be a game-changer. By recycling, we can:\n",
      "\n",
      "* Protect our air and water from pollution, ensuring a healthy environment for generations\n",
      "\n",
      "Final Refined Output:\n",
      "Here's a revised version of the output:\n",
      "\n",
      "**Join the Movement to Save Our Planet: Make Recycling a Habit Today and Secure a Sustainable Future!**\n",
      "\n",
      "The fate of our planet is in our hands. Recycling is not just a responsible practice, it's a matter of survival. By recycling, we can dramatically reduce the staggering amounts of waste that threaten our environment, public health, and the well-being of our communities. Think of the impact we can make: conserving precious water, energy, and raw materials, reducing the environmental footprint of production, transportation, and disposal, and fighting climate change by decreasing the demand for resource-intensive production processes.\n",
      "\n",
      "**The Stakes Are Higher Than Ever**\n",
      "\n",
      "The consequences of inaction are dire. Rising temperatures, devastating natural disasters, and unpredictable weather patterns are all symptoms of a planet in crisis. But the good news is that recycling can be a game-changer. By recycling, we can:\n",
      "\n",
      "* Protect our air and water from pollution, ensuring a healthy environment for generations\n"
     ]
    }
   ],
   "source": [
    "# Initial task prompt\n",
    "initial_prompt = \"Write a short paragraph about the importance of recycling.\"\n",
    "\n",
    "# Instruction to refine the output\n",
    "refinement_instructions = \"Make it more persuasive and add a strong call to action.\"\n",
    "\n",
    "# Call the function\n",
    "final_output = iterative_prompting(initial_prompt, refinement_instructions)\n",
    "\n",
    "print(\"\\nFinal Refined Output:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e1400",
   "metadata": {},
   "source": [
    "## Prompt Engineering Best Practices \n",
    "\n",
    "#### 1. Clarity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a12f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad prompt \n",
    "\"Tell me about dogs\"\n",
    "\n",
    "# Good Prompt \n",
    "\"Provide a comprehensive overview of dog training techniques for first-time dog owners, focusing on house training, basic commands, and socialization. Include specific steps and timeframes. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48089307",
   "metadata": {},
   "source": [
    "### 2. Context setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "417ae96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_aware_prompt = PromptTemplate(\n",
    "    input_variables=['user_level', 'topic', 'goal'], \n",
    "    template=\"\"\"\n",
    "    Context: You're teaching {topic} to someone with {user_level} experience. \n",
    "    Goal: Help them {goal}\n",
    "    \n",
    "    Instructions:\n",
    "    - Adjust complexity to their level\n",
    "    - Provide practical, actionable advice\n",
    "    - Include relevant examples\n",
    "    - Anticipate common questions\n",
    "    \n",
    "    Content:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff16f5",
   "metadata": {},
   "source": [
    "### 3. Output Format Specification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ae11a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_specific_prompt = PromptTemplate(\n",
    "    input_variables=['data'],\n",
    "    template=\"\"\"\n",
    "    Analyze the following data and provide insights in this exact format:\n",
    "    \n",
    "    ## Executive Summary \n",
    "    [2-3 sentence overview]\n",
    "    \n",
    "    ## Key Findings \n",
    "    1. [Finding 1 with supporting data]\n",
    "    2. [Finding 2 with supporting data]\n",
    "    3. [Findign 3 with supporting data]\n",
    "    \n",
    "    ## Recomendations\n",
    "    - [Actionable recommendation 1]\n",
    "    - [Actionable recommendation 2]\n",
    "    - [Actionable recommendation 3]\n",
    "    \n",
    "    ## Risk Assesment\n",
    "    [Potential risks and mitigation strategies]\n",
    "    \n",
    "    Data to analyze:\n",
    "    {data}\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8ee85",
   "metadata": {},
   "source": [
    "### 4. Error Prevention and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c35b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_prompt = PromptTemplate(\n",
    "    input_variables=['user_input'], \n",
    "    template=\"\"\"\n",
    "    Please analyze the following input and provide a response:\n",
    "    \n",
    "    Input: {user_input}\n",
    "    \n",
    "    Guidelines:\n",
    "    1. If the input is unclear, ask for clarification\n",
    "    2. If you cannot provide a complete answer, explain what information is missing\n",
    "    3. Always provide your confidence level (High/Medium/Low)\n",
    "    4. If making assumptions, clearly state them \n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ac731",
   "metadata": {},
   "source": [
    "### 5. Multi-Step Reasoning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_framework = PromptTemplate(\n",
    "    input_variables=['problem'], \n",
    "    template=\"\"\"\n",
    "    Problem: {problem}\n",
    "    \n",
    "    Please solve this using the following framework:\n",
    "    \n",
    "    **UNDERSTAND**: What is the core problem/question?\n",
    "    \n",
    "    **ANALYZE**: What information do I have? What's missing?\n",
    "    \n",
    "    **APPROACH**: What method/strategy should I use?\n",
    "    \n",
    "    **EXECUTE**: Step-by-step solution\n",
    "    \n",
    "    **VERITY**: Does this answer make sense? Any edge cases?\n",
    "    \n",
    "    **CONCLUDE**: Final answer with confidence level\n",
    "    \n",
    "    Let's work through this:\n",
    "    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb7f6a",
   "metadata": {},
   "source": [
    "### Advanced Parameter Turning\n",
    "#### Temperature Guidelines\n",
    "- **0.0-0.3**: Deterministic, factual responses, code Generation\n",
    "- **0.4-0.7**: Balanced creativity and accuracy, general conversation\n",
    "- **0.8-1.0**: Creative writing, brainstorming, artistic tasks\n",
    "\n",
    "\n",
    "#### Top-p (Nucleus Sampling)\n",
    "- **0.1-0.3**: Very Focused vocabulary \n",
    "- **0.5-0.7**: Balanced vocabulary selection\n",
    "- **0.8-0.95**: Diverse vocabulary, creative responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062980d",
   "metadata": {},
   "source": [
    "### Max Tokens Strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be046049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_token_strategy(task_type):\n",
    "    \"\"\"Adjust max_token based on task requirements\"\"\"\n",
    "    token_strategies = {\n",
    "        \"summary\": 150, \n",
    "        'explation':300, \n",
    "        'analysis': 500, \n",
    "        'creative_writing': 1000, \n",
    "        'code_generation': 800, \n",
    "        'brainstoming': 400\n",
    "    }\n",
    "    return token_strategies.get(task_type, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05606fe8",
   "metadata": {},
   "source": [
    "### Performance Optimization Tips\n",
    "\n",
    "##### 1. Prompt Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9a186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def get_cached_response(prompt_text, temperature=0.7):\n",
    "    \"\"\"Cache responses for identical prompts\"\"\"\n",
    "    chain = LLMChain(llm=llm, prompt=PromptTemplate(\n",
    "        input_variables=[], \n",
    "        template=prompt_text\n",
    "    ))\n",
    "    return chain.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb41f1",
   "metadata": {},
   "source": [
    "### 2. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_prompts(prompts, batch_size=5):\n",
    "    \"\"\"Process mutiple prompts efficiently\"\"\"\n",
    "    result = []\n",
    "    for i in range(0, len(prompt), batch_size):\n",
    "        \n",
    "        batch = prompts[i:i+batch_size]\n",
    "        # Process bach here \n",
    "        batch_results = [process_single_prompt(p) for p in batch]\n",
    "        result.extend(batch_size)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b2c46",
   "metadata": {},
   "source": [
    "### 3. Error Handling and Retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e751dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occureed: Missing some input keys: {'contraints', 'topic'}\n",
      "Error occureed: Missing some input keys: {'contraints', 'topic'}\n",
      "Error occureed: Missing some input keys: {'contraints', 'topic'}\n",
      "Failed after retries: RetryError[<Future at 0x119c72bd0 state=finished raised ValueError>]\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def robust_llm_call(chain, **kwargs):\n",
    "    \"\"\"LLM call with retry logic\"\"\"\n",
    "    try:\n",
    "        return chain.run(**kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occureed: {e}\")\n",
    "        raise\n",
    "    \n",
    "# Usage \n",
    "try:\n",
    "    result = robust_llm_call(chain, text = \"kuch bhi puch lo jo me kare\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed after retries: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef4740",
   "metadata": {},
   "source": [
    "## Measuring Prompt Effectiveness \n",
    "\n",
    "#### 1. Response Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14688582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_quality(prompt, expected_elements):\n",
    "    \"\"\"Evaluate if response conatins expected elements\"\"\"\n",
    "    response = chain.run(prompt)\n",
    "    \n",
    "    score = 0 \n",
    "    for element in expected_elements:\n",
    "        if element.lower() in response.lower():\n",
    "            score += 1\n",
    "            \n",
    "    return {\n",
    "        'response': response, \n",
    "        'score': score / len(expected_elements), \n",
    "        'missing_elements': [e for e in expected_elements if e.lower() not in response.lower() ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b23f6",
   "metadata": {},
   "source": [
    "### A/B Testing Framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_test_prompt(prompt_a, prompt_b, test_cases, evaluation_func):\n",
    "    \"\"\"Compare two prompts across multiple test cases\"\"\"\n",
    "    results_b = []\n",
    "    results_c = []\n",
    "    \n",
    "    for test in test_cases:\n",
    "        results_b = evaluation_func(prompt_a.format(**test_cases))\n",
    "        results_c = evaluation_func(prompt_b.format(**test_cases))\n",
    "        \n",
    "        results_b.append(results_b)\n",
    "        results_c.append(results_c)\n",
    "        \n",
    "        return {\n",
    "            'prompt_a_avg': sum(results_b) / len(results_b),\n",
    "            'prompt_b_avg': sum(results_c) / len(results_c),\n",
    "            'winner': 'A' if sum(results_b) > sum(results_c) else 'B'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b007b9",
   "metadata": {},
   "source": [
    "### Common Prompting Pitfalls to Avoid\n",
    "\n",
    "1. **Being Too Vague**: \"Explain AI\" vs \"Explain how neural networks process image data\"\n",
    "2. **Not Setting Context**: Always provide background information\n",
    "3. **Ignoring Output Format**: Specify exactly how you want the response structured\n",
    "4. **Not Testing Edge Cases**: Test with unusual or difficult inputs\n",
    "5. **Overlooking Bias**: Be aware of potential biases in responses\n",
    "6.** Not Iterating**: Prompts should be refined based on results\n",
    "\n",
    "#### Conclusion\n",
    "Effective prompting is crucial for getting the best results from LLMs. Start with clear, specific prompts and gradually add complexity. Always test different approaches and measure the results to continuously improve your prompting strategy.\n",
    "\n",
    "\n",
    "Remember: The key to great prompting is understanding both your model's capabilities and your specific use case requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed322e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
