{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811e6768",
   "metadata": {},
   "source": [
    "## Data Ingestion & Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b3d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader, PyPDFLoader\n",
    "from langchain_community.document_loaders.powerpoint import UnstructuredPowerPointLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9aa117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing documents \n",
    "directory_path = 'data/'\n",
    "\n",
    "txt_loader = DirectoryLoader(\n",
    "    path=directory_path,\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "\n",
    "pdf_loader = DirectoryLoader(\n",
    "    path=directory_path,\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader  \n",
    ")\n",
    "\n",
    "\n",
    "pptx_loader = DirectoryLoader( \n",
    "    path=directory_path,\n",
    "    glob=\"**/*.pptx\",\n",
    "    loader_cls=UnstructuredPowerPointLoader\n",
    ")\n",
    "\n",
    "\n",
    "# Load documents from each loader \n",
    "\n",
    "txt_docs = txt_loader.load()\n",
    "pdf_docs = pdf_loader.load()\n",
    "pptx_docs =pptx_loader.load()\n",
    "\n",
    "\n",
    "# Combine all documents into a single list\n",
    "all_docs = txt_docs + pdf_docs + pptx_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed37380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,        # each chunk will be ~1000 characters\n",
    "    chunk_overlap=80       # 200 characters will overlap between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b37c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e7e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "directory_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da6f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Text_Loader(dir:Path):\n",
    "    # Define the directory containing documents \n",
    "\n",
    "    txt_loader = DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "\n",
    "\n",
    "    pdf_loader = DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader  \n",
    "    )\n",
    "\n",
    "\n",
    "    pptx_loader = DirectoryLoader( \n",
    "        path=directory_path,\n",
    "        glob=\"**/*.pptx\",\n",
    "        loader_cls=UnstructuredPowerPointLoader\n",
    "    )\n",
    "\n",
    "\n",
    "    # Load documents from each loader \n",
    "\n",
    "    txt_docs = txt_loader.load()\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    pptx_docs =pptx_loader.load()\n",
    "\n",
    "\n",
    "    # Combine all documents into a single list\n",
    "    all_docs = txt_docs + pdf_docs + pptx_docs\n",
    "    \n",
    "    return all_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a977dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_splitter(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,        # each chunk will be ~1000 characters\n",
    "        chunk_overlap=80       # 200 characters will overlap between chunks\n",
    "    )\n",
    "\n",
    "    docs = splitter.split_documents(docs)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17088ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = Text_Loader(directory_path)\n",
    "docs = doc_splitter(Text_Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51231c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Loader = Text_Loader(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1068476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "243a655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adef3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76142a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0,\n",
    "    api_key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01be9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Assume you already have `embedding_model` and `docs` (a list of chunked Document objects)\n",
    "\n",
    "# Create FAISS index\n",
    "db = FAISS.from_documents(documents=docs, embedding=embedding_model)\n",
    "\n",
    "# (Optional) Save the FAISS index locally\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "# (Optional) Load it back later\n",
    "# db = FAISS.load_local(\"faiss_index\", embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ef9a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS version\n",
    "db = FAISS.from_documents(documents=docs, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1533821",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = db.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "492230b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be6a8a",
   "metadata": {},
   "source": [
    "## PROMPT DESIGNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a4343ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    Your a an AI researcher who is an expert in RAG systems.\n",
    "    Answer any question asked by the user.\n",
    "    construct answers in the form of bullet points\n",
    "    Craft your response only from the provided context only.\n",
    "    If you cannot find any related information from the context, simply say no context provied.\n",
    "    Do not hallucinate.\n",
    "    \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    QUESTION:{question}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f66000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554fbf4",
   "metadata": {},
   "source": [
    "## GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80cff1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No context provided.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = input(str())\n",
    "relevant_info = retriver.invoke(user_prompt)# Get relevent info from db and will be stuffed into the prompt as {context}.\n",
    "response = document_chain.invoke({\"context\": relevant_info, \"question\": user_prompt})# feed related docs and user query to model.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b632941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64d9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0a48f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This process ensures that the generated text is not only contextually accurate but also grounded in factual information, making RAG particularly useful for applications such as question answering, document summarization, and legal assistant tools.\\n\\nAdvanced RAG Techniques'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[10].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "186a8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = txt_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14a3097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
      "\n",
      "RAG and Advanced RAG: Concepts and Improvements\n",
      "\n",
      "Introduction\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is an advanced technique in natural language processing that combines the power of language models with external knowledge sources. Unlike traditional text generation, which relies solely on pre-trained models, RAG integrates a retrieval mechanism that enables the model to access external documents, databases, or knowledge sources to enhance the accuracy and relevance of generated text. This approach bridges the gap between static language models and dynamic, knowledge-driven responses.\n",
      "\n",
      "Understanding RAG\n",
      "\n",
      "RAG operates through a two-step process:\n",
      "\n",
      "1. Retrieval Phase: The model first retrieves relevant documents or information from an external database or corpus. This retrieval is often handled by a dense passage retriever (DPR) or other retrieval models, which identify the most relevant content based on the input query.\n",
      "\n",
      "2. Generation Phase: The retrieved information is then passed to a generative model (typically a transformer-based model) that generates a coherent and contextually accurate response using the retrieved knowledge.\n",
      "\n",
      "This process ensures that the generated text is not only contextually accurate but also grounded in factual information, making RAG particularly useful for applications such as question answering, document summarization, and legal assistant tools.\n",
      "\n",
      "Advanced RAG Techniques\n",
      "\n",
      "While standard RAG provides a robust framework for knowledge-augmented generation, several advanced techniques have been developed to further enhance its performance:\n",
      "\n",
      "1. Context-Aware Retrieval: This involves using query expansion, query rewriting, or user history to improve retrieval accuracy. It ensures that the most contextually relevant documents are retrieved.\n",
      "\n",
      "2. Adaptive Retrieval Mechanisms: Instead of using a static retrieval method, the system can adaptively switch between dense and sparse retrieval methods based on the query type, improving flexibility.\n",
      "\n",
      "3. Multi-Document Synthesis: Rather than relying on a single retrieved document, advanced RAG can synthesize information from multiple sources, providing more accurate and comprehensive responses.\n",
      "\n",
      "4. Enhanced Generation Control: By using advanced prompt engineering, templates, or conditional generation methods, the output can be fine-tuned for tone, style, or focus, making it suitable for diverse applications.\n",
      "\n",
      "5. Knowledge Graph Integration: Integrating RAG with structured knowledge graphs allows the system to leverage entity relationships, providing more precise and context-rich answers.\n",
      "\n",
      "Improvements in RAG Systems\n",
      "\n",
      "Several improvements can be made to RAG systems to enhance their accuracy, scalability, and efficiency:\n",
      "\n",
      "1. Dynamic Knowledge Updating: Regularly updating the knowledge base ensures that the system provides the most accurate and up-to-date information.\n",
      "\n",
      "2. Enhanced Retrieval Models: Using state-of-the-art retrievers such as ColBERT, BM25, or hybrid retrievers can significantly improve retrieval accuracy.\n",
      "\n",
      "3. Fine-Tuning for Specific Domains: Tailoring RAG models for specific domains (such as law, healthcare, or education) ensures that they generate more accurate and contextually relevant responses.\n",
      "\n",
      "4. Optimized Storage Solutions: Implementing efficient storage techniques (such as FAISS or HNSW) for large-scale knowledge bases ensures fast and scalable retrieval.\n",
      "\n",
      "5. Advanced Error Handling Mechanisms: Introducing fallback mechanisms in case of retrieval failure or poor generation quality can enhance user experience.\n",
      "\n",
      "Applications of RAG and Advanced RAG\n",
      "\n",
      "RAG has found wide applications across various domains, including:\n",
      "\n",
      "1. Legal Document Analysis: Assisting in the analysis and summarization of complex legal documents.\n",
      "2. Healthcare Consultation: Providing accurate medical advice by retrieving information from verified sources.\n",
      "3. Customer Support Automation: Enhancing chatbot capabilities with real-time, context-aware responses.\n",
      "4. Educational Platforms: Offering personalized learning assistance based on external knowledge sources.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "RAG and Advanced RAG represent a significant leap in natural language processing, bridging the gap between static models and dynamic, knowledge-driven responses. By continuously refining retrieval mechanisms, integrating domain-specific knowledge, and improving generation control, RAG systems can be further enhanced to meet the diverse needs of modern applications.\n",
      "\n",
      "{'source': 'data/RAG research paper.txt'}\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc.page_content)     # Prints the text\n",
    "    print(doc.metadata)         # Shows file info like 'source'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3bd42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
